{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigQuery and Dataflow (Apache Beam on GCP)\n",
    "This is an excerpt from a dataflow pipeline project built for an NLP client. We needed to tokenize the (massive) [Google Patents](https://cloud.google.com/blog/topics/public-datasets/google-patents-public-datasets-connecting-public-paid-and-private-patent-data) dataset, for later training of several custom transformers models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -U spacy\n",
    "!{sys.executable} -m pip install -U spacy-lookups-data\n",
    "!{sys.executable} -m spacy download en\n",
    "!{sys.executable} -m pip install nltk\n",
    "!{sys.executable} -m pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.options import pipeline_options\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.runners import DataflowRunner\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "from apache_beam.io.gcp.internal.clients import bigquery\n",
    "\n",
    "import google.auth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup (Omitted)\n",
    "Contains proprietary cloud infrastructure information. Redacted out of an abundance of caution. Effectively, at this step I:\n",
    "1) Instantiate a [pipeline_options.PipelineOptions](https://beam.apache.org/releases/pydoc/2.5.0/apache_beam.options.pipeline_options.html) object.\n",
    "2) Configure its GCP project, zone, and cloud storage bucket for the client's project.\n",
    "3) Set the Google Cloud Storage output bucket for the pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigQuery Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.io.gcp.internal.clients import bigquery\n",
    "import pyarrow as pa\n",
    "import pickle\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "\n",
    "table_spec = bigquery.TableReference(\n",
    "    projectId=os.getenv('CLIENT_PROJECT_ID'),\n",
    "    datasetId=os.getenv('CLIENT_BQ_DATASET'),\n",
    "    tableId=os.getenv('CLIENT_BQ_TABLE'))\n",
    "\n",
    "\n",
    "fields = [\n",
    "    ('block', pa.binary())\n",
    "]\n",
    "schema = pyarrow.schema(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define One or Multiple Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(e):\n",
    "    '''\n",
    "    A basic text cleaning function on Big Query patent objects that \n",
    "    casts everything to lowercase and removes any redundant whitespace.\n",
    "    '''\n",
    "    jdata = json.loads(e)\n",
    "    txt = jdata['abs_text'] + jdata['desc_text'] + jdata['claims_text']\n",
    "    low = txt.lower()\n",
    "    alpha = re.sub(r'[^A-Za-z\\s]+', '', low)\n",
    "    rm_space = re.sub(r'[\\s]{2,}', ' ', alpha)\n",
    "    return rm_space\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(InteractiveRunner(), options=options)\n",
    "\n",
    "#Instantiate a Transformers tokenizer object using the GPT2 presets\n",
    "tok = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "from_table = (\n",
    "    p\n",
    "    | 'ReadTable' >> beam.io.Read(beam.io.BigQuerySource(table_spec))\n",
    "    | 'Tokenize' >> beam.Map(lambda x: tok.encode(x['abs_text'] + ' ' + x['desc_text'] + ' ' + x['claims_text']))\n",
    "    | 'Serialize for Parquet' >> beam.Map(lambda x: {'block' : pickle.dumps(tok.encode(x))})\n",
    "    | 'Write to GCS' >> beam.io.WriteToParquet(os.path.join(output_gcs_location, f'pq_out'), schema)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optionally, Inspect the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show_graph(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result = DataflowRunner().run_pipeline(p, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "url = ('https://console.cloud.google.com/dataflow/jobs/%s/%s?project=%s' % \n",
    "      (pipeline_result._job.location, pipeline_result._job.id, pipeline_result._job.projectId))\n",
    "display(HTML('Click <a href=\"%s\" target=\"_new\">here</a> for the details of your Dataflow job!' % url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check\n",
    "Once things are done, download one of the output parquet files and load it. Then, decode one of the blocks to make sure the tokenization successfully completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "a = pq.read_table('pq_out-00000-of-00001')\n",
    "\n",
    "arr = a['block']\n",
    "\n",
    "tok.decode(pickle.loads(a['block'][0].as_py()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
